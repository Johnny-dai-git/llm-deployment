#!/bin/bash
set -e

# ======= é…ç½®åŒºåŸŸ =======
GITHUB_USERNAME="Johnny-dai-git"
GITHUB_TOKEN="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"
GITHUB_REPO="llm-deployment"
GITHUB_BRANCH="main"
GITHUB_URL="https://${GITHUB_TOKEN}@github.com/${GITHUB_USERNAME}/${GITHUB_REPO}.git"

# æŒä¹…åŒ–å­˜å‚¨è®¾å¤‡ï¼ˆåŠ¨æ€æ£€æµ‹æŒ‚è½½ç‚¹ï¼‰
STORAGE_DEVICE="/dev/sda4"

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_DIR="${SCRIPT_DIR}"
INSTALL_DIR="${REPO_DIR}/install"
CONTROL_DIR="${REPO_DIR}/tools"

echo "===== æœ¬åœ° system èŠ‚ç‚¹å®Œæ•´åˆå§‹åŒ–å¼€å§‹ï¼ˆcontrol planeï¼‰====="

# ================================================================
# Phase 0: ç¡®ä¿ git å·²å®‰è£…
# ================================================================
echo "===== Phase 0: ç¡®ä¿ git å·²å®‰è£… ====="
which git || (sudo apt update && sudo apt install -y git)

# ================================================================
# Phase 1: æ›´æ–° GitHub ä»“åº“
# ================================================================
echo "===== Phase 1: æ›´æ–° GitHub ä»“åº“ ====="
cd "${REPO_DIR}"
if [ -d ".git" ]; then
  git pull origin ${GITHUB_BRANCH} || echo "âš  Git pull failed, continuing..."
fi

# ================================================================
# Phase 2: é€šç”¨åˆå§‹åŒ–
# ================================================================
echo "===== Phase 2: æ‰§è¡Œ all_install.sh ====="
cd "${INSTALL_DIR}"
sudo bash all_install.sh

# ================================================================
# Phase 3: åˆå§‹åŒ– Kubernetes æ§åˆ¶å¹³é¢
# ================================================================
echo "===== Phase 3: æ‰§è¡Œ system.sh ====="
sudo bash system.sh

# ================================================================
# Phase 4: é›†ç¾¤åŸºç¡€è®¾æ–½ + GPU Bootstrap
# ================================================================
echo "===== Phase 4: é›†ç¾¤åŸºç¡€è®¾æ–½ + GPU Bootstrap ====="

# ------------------------------------------------
# Step 0: NVIDIA Device Plugin
# ------------------------------------------------
echo "===== Step 0: Install NVIDIA Device Plugin ====="

if [ -f "${CONTROL_DIR}/system/nvidia-device-plugin.yaml" ]; then
  kubectl apply -f "${CONTROL_DIR}/system/nvidia-device-plugin.yaml"
elif [ -f "${REPO_DIR}/script/nvidia-device-plugin.yaml" ]; then
  kubectl apply -f "${REPO_DIR}/script/nvidia-device-plugin.yaml"
else
  echo "âŒ nvidia-device-plugin.yaml not found"
  exit 1
fi

echo ">>> Waiting for NVIDIA device plugin to be ready..."
kubectl rollout status ds/nvidia-device-plugin-daemonset -n kube-system --timeout=60s || \
  echo "âš  Device plugin rollout may still be in progress, continuing..."
sleep 2
kubectl describe node system | grep -A4 nvidia.com/gpu || \
  echo "âš  GPU not visible yet, continue..."

# ------------------------------------------------
# Step 0.5: Ensure NVIDIA RuntimeClass (CRITICAL)
# ------------------------------------------------
echo "===== Step 0.5: Ensure NVIDIA RuntimeClass ====="

RUNTIMECLASS_YAML="${CONTROL_DIR}/system/runtimeclass-nvidia.yaml"

if [ -f "${RUNTIMECLASS_YAML}" ]; then
  kubectl get runtimeclass nvidia >/dev/null 2>&1 || \
    kubectl apply -f "${RUNTIMECLASS_YAML}"
  echo "âœ” RuntimeClass nvidia created/verified from ${RUNTIMECLASS_YAML}"
else
  echo ">>> runtimeclass-nvidia.yaml not found, creating inline..."
  cat <<EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF
fi

kubectl get runtimeclass nvidia
echo ">>> RuntimeClass nvidia details:"
kubectl get runtimeclass nvidia -o yaml | grep -A2 "handler:"

# ------------------------------------------------
# Step 1: Namespaces / Node labels
# ------------------------------------------------
cd "${CONTROL_DIR}"

kubectl apply -f base/namespaces/

SYSTEM_NODE="system"
kubectl label node ${SYSTEM_NODE} system=true ingress=true gpu-node=true --overwrite

# ------------------------------------------------
# Step 1.2: å®‰è£…å’Œé…ç½® local-path-provisionerï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
# ------------------------------------------------
echo "===== Step 1.2: Install and Configure local-path-provisioner ====="

# åŠ¨æ€æ£€æµ‹å­˜å‚¨è®¾å¤‡çš„æŒ‚è½½ç‚¹
echo ">>> Detecting mount point for ${STORAGE_DEVICE}..."
MOUNT_POINT=$(findmnt -n -o TARGET "${STORAGE_DEVICE}" 2>/dev/null || \
              mount | grep "${STORAGE_DEVICE}" | awk '{print $3}' | head -1)

if [ -z "${MOUNT_POINT}" ]; then
  echo "âš ï¸  Warning: ${STORAGE_DEVICE} is not mounted, trying alternative detection..."
  # å°è¯•é€šè¿‡ lsblk è·å–æŒ‚è½½ç‚¹
  MOUNT_POINT=$(lsblk -n -o MOUNTPOINT "${STORAGE_DEVICE}" 2>/dev/null | grep -v "^$" | head -1)
fi

if [ -z "${MOUNT_POINT}" ]; then
  echo "âŒ Error: Cannot detect mount point for ${STORAGE_DEVICE}"
  echo "   Please ensure ${STORAGE_DEVICE} is mounted before running this script"
  exit 1
fi

LOCAL_STORAGE_PATH="${MOUNT_POINT}/k8s"
echo ">>> Detected mount point: ${MOUNT_POINT}"
echo ">>> Using storage path: ${LOCAL_STORAGE_PATH}"

# Step 1.2.1: å®‰è£… local-path-provisionerï¼ˆå®Œå…¨é‡è£…ä»¥ç¡®ä¿ ConfigMap å®Œæ•´ï¼‰
echo ">>> Installing local-path-provisioner..."
if kubectl get namespace local-path-storage >/dev/null 2>&1; then
  echo ">>> Removing existing local-path-storage to ensure clean installation..."
  kubectl delete namespace local-path-storage --wait=true --timeout=60s || \
    echo "âš  Namespace deletion may still be in progress, continuing..."
  sleep 5
fi

# ä½¿ç”¨å®˜æ–¹å®Œæ•´ YAML å®‰è£…ï¼ˆåŒ…å« helperPod.yamlï¼‰
echo ">>> Applying official local-path-provisioner YAML..."
kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
echo ">>> Waiting for local-path-provisioner to be ready..."
sleep 10
kubectl wait --for=condition=ready pod -l app=local-path-provisioner -n local-path-storage --timeout=60s || \
  echo "âš  local-path-provisioner may still be starting..."

# Step 1.2.2: åœ¨å®¿ä¸»æœºåˆ›å»ºç›®å½•å¹¶æˆæƒ
echo ">>> Creating storage directory: ${LOCAL_STORAGE_PATH}"
sudo mkdir -p "${LOCAL_STORAGE_PATH}"
sudo chown -R root:root "${LOCAL_STORAGE_PATH}"
sudo chmod 755 "${LOCAL_STORAGE_PATH}"

# Step 1.2.3: é…ç½® local-path ä½¿ç”¨æŒ‡å®šçš„ç£ç›˜è·¯å¾„ï¼ˆåªæ›´æ–° config.jsonï¼Œä¿ç•™ helperPod.yamlï¼‰
echo ">>> Configuring local-path-provisioner to use: ${LOCAL_STORAGE_PATH}"
# ç­‰å¾… ConfigMap åˆ›å»ºå®Œæˆ
sleep 3

# éªŒè¯ ConfigMap æ˜¯å¦åŒ…å« helperPod.yaml
if ! kubectl get configmap local-path-config -n local-path-storage -o jsonpath='{.data.helperPod\.yaml}' 2>/dev/null | grep -q .; then
  echo "âš ï¸  Warning: helperPod.yaml not found in ConfigMap"
  echo "   Re-applying official YAML to restore helperPod.yaml..."
  kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
  sleep 3
fi

# åªæ›´æ–° config.json å­—æ®µï¼Œä¿ç•™å…¶ä»–å­—æ®µï¼ˆå¦‚ helperPod.yamlï¼‰
# ä½¿ç”¨ kubectl patch åªæ›´æ–° config.json
kubectl patch configmap local-path-config -n local-path-storage --type merge -p "{\"data\":{\"config.json\":\"{\\\"nodePathMap\\\":[{\\\"node\\\":\\\"DEFAULT_PATH_FOR_NON_LISTED_NODES\\\",\\\"paths\\\":[\\\"${LOCAL_STORAGE_PATH}\\\"]}]}\"}}"

# Step 1.2.4: é‡å¯ local-path-provisioner ä½¿é…ç½®ç”Ÿæ•ˆ
echo ">>> Restarting local-path-provisioner to apply new configuration..."
kubectl rollout restart daemonset local-path-provisioner -n local-path-storage 2>/dev/null || \
  kubectl delete pod -l app=local-path-provisioner -n local-path-storage 2>/dev/null || true
sleep 5

# Step 1.2.5: è®¾ç½® local-path ä¸ºé»˜è®¤ StorageClass
echo ">>> Setting local-path as default StorageClass..."
kubectl patch storageclass local-path \
  -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' 2>/dev/null || \
  echo "âš  Failed to patch storageclass, may already be default"

# ç§»é™¤å…¶ä»– StorageClass çš„é»˜è®¤æ ‡è®°ï¼ˆå¦‚æœæœ‰ï¼‰
kubectl get storageclass -o name 2>/dev/null | grep -v local-path | xargs -I {} kubectl patch {} \
  -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' 2>/dev/null || true

# éªŒè¯
echo ">>> Verifying StorageClass configuration..."
kubectl get storageclass
kubectl get pods -n local-path-storage || echo "âš  local-path-storage pods may still be starting..."

# ------------------------------------------------
# Step 1.5: æ·»åŠ  Helm ä»“åº“
# ------------------------------------------------
echo "===== Step 1.5: æ·»åŠ  Helm ä»“åº“ ====="
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
helm repo add nvidia https://nvidia.github.io/dcgm-exporter/helm-charts || true
helm repo update
# ------------------------------------------------
# Step 2: ingress-nginx
# ------------------------------------------------
# ç»Ÿä¸€é…ç½®ï¼šæ‰€æœ‰æƒ…å†µéƒ½ä½¿ç”¨ hostNetwork: true
echo ">>> é…ç½® ingress-nginxï¼šä½¿ç”¨ hostNetworkï¼ˆç»Ÿä¸€é…ç½®ï¼‰"

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx || true
helm repo update

helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx  \
  --namespace ingress-nginx  \
  --create-namespace  \
  --set controller.hostNetwork=true  \
  --set controller.dnsPolicy=ClusterFirstWithHostNet  \
  --set controller.service.type=ClusterIP

# ç­‰å¾… ingress-nginx å®Œå…¨å°±ç»ªï¼ˆç”Ÿäº§çº§åšæ³•ï¼‰
# âš ï¸ é‡è¦ï¼šå¿…é¡»ç­‰å¾… admission webhook Ready æ‰èƒ½åˆ›å»º Ingress èµ„æº
# å¦åˆ™ Ingress åˆ›å»ºä¼šå¤±è´¥ï¼ˆwebhook æœªå°±ç»ªï¼Œæ— æ³•éªŒè¯ Ingress èµ„æºï¼‰
echo ">>> Waiting for ingress-nginx controller to be ready..."
kubectl rollout status deployment ingress-nginx-controller \
  -n ingress-nginx --timeout=120s || echo "âš  Controller rollout may still be in progress..."

# ç­‰å¾… admission webhook configurationï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
# âš ï¸ æ³¨æ„ï¼šadmission webhook æ˜¯ ValidatingWebhookConfigurationï¼Œä¸æ˜¯ Deployment
# Webhook = API Server è°ƒç”¨ Serviceï¼Œéœ€è¦ç­‰å¾… WebhookConfiguration å‡ºç°
echo ">>> Waiting for ingress-nginx admission webhook configuration..."
until kubectl get validatingwebhookconfiguration ingress-nginx-admission >/dev/null 2>&1; do
  echo "  Waiting for webhook configuration..."
  sleep 2
done
echo "âœ” Admission webhook configuration ready"

# ------------------------------------------------
# Step 3: ArgoCD
# ------------------------------------------------
kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
sleep 15
kubectl apply -f argocd/argocd-ingress.yaml

# æ³¨æ„ï¼šGrafana / Prometheus / ArgoCD å·²ä½¿ç”¨ Ingress + å­è·¯å¾„æ¶æ„
# æ‰€æœ‰æœåŠ¡çš„ root_url éƒ½ä½¿ç”¨ %(protocol)s://%(domain)s/<path>/ æ¨¡å¼
# ä¸éœ€è¦åŠ¨æ€è·å–æˆ–æ›¿æ¢ IP åœ°å€


# å®‰è£… ArgoCD Image Updaterï¼ˆä½¿ç”¨ YAML manifestï¼‰
# å…ˆåˆ é™¤æ‰€æœ‰ç°æœ‰çš„ Image Updater Deploymentï¼ˆé¿å…å†²çªï¼‰
echo ">>> Removing existing Image Updater Deployments to avoid conflict..."
kubectl delete deployment -n argocd argocd-image-updater argocd-image-updater-controller --ignore-not-found=true
sleep 3

# å®‰è£…åŸºç¡€èµ„æºï¼ˆConfigMapã€ServiceAccountã€RBAC ç­‰ï¼‰
echo ">>> Installing ArgoCD Image Updater base resources..."
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/v0.15.2/manifests/install.yaml || \
  echo "âš  Official install.yaml may have failed, continuing with custom Deployment..."

# å†æ¬¡åˆ é™¤å®˜æ–¹ Deploymentï¼ˆinstall.yaml ä¼šåˆ›å»ºå®ƒï¼Œä½†æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„ï¼‰
echo ">>> Removing official Deployment (we use custom one)..."
kubectl delete deployment -n argocd argocd-image-updater --ignore-not-found=true
sleep 2

# åº”ç”¨è‡ªå®šä¹‰çš„ Deploymentï¼ˆä¿®å¤äº† command/args é—®é¢˜ï¼‰
echo ">>> Applying custom ArgoCD Image Updater Deployment..."
kubectl apply -f system/argocd-image-updater-controller.yaml

echo ">>> Waiting for Image Updater to be ready..."
sleep 10
kubectl get pods -n argocd | grep image-updater || echo "âš  Image Updater pods may still be starting..."

# ------------------------------------------------
# Step 3.6: å®‰è£… Monitoring Stack (Helm)
# ------------------------------------------------
echo "===== Step 3.6: Install Monitoring Stack (Helm) ====="

# ç¡®ä¿ Helm repo å·²æ·»åŠ ï¼ˆStep 1.5 å·²æ·»åŠ ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
helm repo add nvidia https://nvidia.github.io/dcgm-exporter/helm-charts || true
helm repo update

# å®‰è£… kube-prometheus-stack (Grafana + Prometheus)
echo ">>> Installing kube-prometheus-stack..."
helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
  -n monitoring \
  --create-namespace \
  -f "${REPO_DIR}/helm/monitoring/kps-values.yaml" \
  --wait --timeout 10m || \
  echo "âš  kube-prometheus-stack installation may still be in progress..."

# å®‰è£… DCGM exporter (GPU metrics)
echo ">>> Installing dcgm-exporter..."
helm upgrade --install dcgm nvidia/dcgm-exporter \
  -n monitoring \
  -f "${REPO_DIR}/helm/monitoring/dcgm/values.yaml" \
  --wait --timeout 5m || \
  echo "âš  dcgm-exporter installation may still be in progress..."

echo ">>> Checking monitoring pods..."
kubectl get pods -n monitoring || echo "âš  Monitoring pods may still be starting..."

# ------------------------------------------------
# Step 4: çŠ¶æ€æ£€æŸ¥
# ------------------------------------------------
kubectl get runtimeclass
kubectl get nodes -o wide
kubectl get pods -A -o wide

echo ""
echo "ğŸ‰ GPU-ready Kubernetes cluster bootstrap å®Œæˆ"