#!/bin/bash
set -e

# ======= é…ç½®åŒºåŸŸ =======
GITHUB_USERNAME="Johnny-dai-git"
GITHUB_TOKEN="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"
GITHUB_REPO="llm-deployment"
GITHUB_BRANCH="main"
GITHUB_URL="https://${GITHUB_TOKEN}@github.com/${GITHUB_USERNAME}/${GITHUB_REPO}.git"

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_DIR="${SCRIPT_DIR}"
INSTALL_DIR="${REPO_DIR}/install"
CONTROL_DIR="${REPO_DIR}/tools"

echo "===== æœ¬åœ° system èŠ‚ç‚¹å®Œæ•´åˆå§‹åŒ–å¼€å§‹ï¼ˆcontrol planeï¼‰====="

# ================================================================
# Phase 0: ç¡®ä¿ git å·²å®‰è£…
# ================================================================
echo "===== Phase 0: ç¡®ä¿ git å·²å®‰è£… ====="
which git || (sudo apt update && sudo apt install -y git)

# ================================================================
# Phase 1: æ›´æ–° GitHub ä»“åº“
# ================================================================
echo "===== Phase 1: æ›´æ–° GitHub ä»“åº“ ====="
cd "${REPO_DIR}"
if [ -d ".git" ]; then
  git pull origin ${GITHUB_BRANCH} || echo "âš  Git pull failed, continuing..."
fi

# ================================================================
# Phase 2: é€šç”¨åˆå§‹åŒ–
# ================================================================
echo "===== Phase 2: æ‰§è¡Œ all_install.sh ====="
cd "${INSTALL_DIR}"
sudo bash all_install.sh

# ================================================================
# Phase 3: åˆå§‹åŒ– Kubernetes æ§åˆ¶å¹³é¢
# ================================================================
echo "===== Phase 3: æ‰§è¡Œ system.sh ====="
sudo bash system.sh

# ================================================================
# Phase 4: é›†ç¾¤åŸºç¡€è®¾æ–½ + GPU Bootstrap
# ================================================================
echo "===== Phase 4: é›†ç¾¤åŸºç¡€è®¾æ–½ + GPU Bootstrap ====="

# ------------------------------------------------
# Step 0: NVIDIA Device Plugin
# ------------------------------------------------
echo "===== Step 0: Install NVIDIA Device Plugin ====="

if [ -f "${CONTROL_DIR}/system/nvidia-device-plugin.yaml" ]; then
  kubectl apply -f "${CONTROL_DIR}/system/nvidia-device-plugin.yaml"
elif [ -f "${REPO_DIR}/script/nvidia-device-plugin.yaml" ]; then
  kubectl apply -f "${REPO_DIR}/script/nvidia-device-plugin.yaml"
else
  echo "âŒ nvidia-device-plugin.yaml not found"
  exit 1
fi

echo ">>> Waiting for NVIDIA device plugin to be ready..."
kubectl rollout status ds/nvidia-device-plugin-daemonset -n kube-system --timeout=60s || \
  echo "âš  Device plugin rollout may still be in progress, continuing..."
sleep 2
kubectl describe node system | grep -A4 nvidia.com/gpu || \
  echo "âš  GPU not visible yet, continue..."

# ------------------------------------------------
# Step 0.5: Ensure NVIDIA RuntimeClass (CRITICAL)
# ------------------------------------------------
echo "===== Step 0.5: Ensure NVIDIA RuntimeClass ====="

RUNTIMECLASS_YAML="${CONTROL_DIR}/system/runtimeclass-nvidia.yaml"

if [ -f "${RUNTIMECLASS_YAML}" ]; then
  kubectl get runtimeclass nvidia >/dev/null 2>&1 || \
    kubectl apply -f "${RUNTIMECLASS_YAML}"
  echo "âœ” RuntimeClass nvidia created/verified from ${RUNTIMECLASS_YAML}"
else
  echo ">>> runtimeclass-nvidia.yaml not found, creating inline..."
  cat <<EOF | kubectl apply -f -
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia
handler: nvidia
EOF
fi

kubectl get runtimeclass nvidia
echo ">>> RuntimeClass nvidia details:"
kubectl get runtimeclass nvidia -o yaml | grep -A2 "handler:"

# ------------------------------------------------
# Step 1: Namespaces / Node labels
# ------------------------------------------------
cd "${CONTROL_DIR}"

kubectl apply -f base/namespaces/

SYSTEM_NODE="system"
kubectl label node ${SYSTEM_NODE} system=true ingress=true gpu-node=true --overwrite


# ------------------------------------------------
# Step 1.5: æ·»åŠ  Helm ä»“åº“
# ------------------------------------------------
echo "===== Step 1.5: æ·»åŠ  Helm ä»“åº“ ====="
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
helm repo add nvidia https://nvidia.github.io/dcgm-exporter/helm-charts || true
helm repo update
# ------------------------------------------------
# Step 2: ingress-nginx
# ------------------------------------------------
# ç»Ÿä¸€é…ç½®ï¼šæ‰€æœ‰æƒ…å†µéƒ½ä½¿ç”¨ hostNetwork: true
echo ">>> é…ç½® ingress-nginxï¼šä½¿ç”¨ hostNetworkï¼ˆç»Ÿä¸€é…ç½®ï¼‰"

helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx || true
helm repo update

helm upgrade --install ingress-nginx ingress-nginx/ingress-nginx  \
  --namespace ingress-nginx  \
  --create-namespace  \
  --set controller.hostNetwork=true  \
  --set controller.dnsPolicy=ClusterFirstWithHostNet  \
  --set controller.service.type=ClusterIP

# ç­‰å¾… ingress-nginx å®Œå…¨å°±ç»ªï¼ˆç”Ÿäº§çº§åšæ³•ï¼‰
# âš ï¸ é‡è¦ï¼šå¿…é¡»ç­‰å¾… admission webhook Ready æ‰èƒ½åˆ›å»º Ingress èµ„æº
# å¦åˆ™ Ingress åˆ›å»ºä¼šå¤±è´¥ï¼ˆwebhook æœªå°±ç»ªï¼Œæ— æ³•éªŒè¯ Ingress èµ„æºï¼‰
echo ">>> Waiting for ingress-nginx controller to be ready..."
kubectl rollout status deployment ingress-nginx-controller \
  -n ingress-nginx --timeout=120s || echo "âš  Controller rollout may still be in progress..."

# ç­‰å¾… admission webhook configurationï¼ˆæ­£ç¡®æ–¹å¼ï¼‰
# âš ï¸ æ³¨æ„ï¼šadmission webhook æ˜¯ ValidatingWebhookConfigurationï¼Œä¸æ˜¯ Deployment
# Webhook = API Server è°ƒç”¨ Serviceï¼Œéœ€è¦ç­‰å¾… WebhookConfiguration å‡ºç°
echo ">>> Waiting for ingress-nginx admission webhook configuration..."
until kubectl get validatingwebhookconfiguration ingress-nginx-admission >/dev/null 2>&1; do
  echo "  Waiting for webhook configuration..."
  sleep 2
done
echo "âœ” Admission webhook configuration ready"

# ------------------------------------------------
# Step 3: ArgoCD
# ------------------------------------------------
kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
sleep 15
kubectl apply -f argocd/argocd-ingress.yaml

# æ³¨æ„ï¼šGrafana / Prometheus / ArgoCD å·²ä½¿ç”¨ Ingress + å­è·¯å¾„æ¶æ„
# æ‰€æœ‰æœåŠ¡çš„ root_url éƒ½ä½¿ç”¨ %(protocol)s://%(domain)s/<path>/ æ¨¡å¼
# ä¸éœ€è¦åŠ¨æ€è·å–æˆ–æ›¿æ¢ IP åœ°å€


# å®‰è£… ArgoCD Image Updaterï¼ˆä½¿ç”¨ YAML manifestï¼‰
# å…ˆåˆ é™¤æ‰€æœ‰ç°æœ‰çš„ Image Updater Deploymentï¼ˆé¿å…å†²çªï¼‰
echo ">>> Removing existing Image Updater Deployments to avoid conflict..."
kubectl delete deployment -n argocd argocd-image-updater argocd-image-updater-controller --ignore-not-found=true
sleep 3

# å®‰è£…åŸºç¡€èµ„æºï¼ˆConfigMapã€ServiceAccountã€RBAC ç­‰ï¼‰
echo ">>> Installing ArgoCD Image Updater base resources..."
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj-labs/argocd-image-updater/v0.15.2/manifests/install.yaml || \
  echo "âš  Official install.yaml may have failed, continuing with custom Deployment..."

# å†æ¬¡åˆ é™¤å®˜æ–¹ Deploymentï¼ˆinstall.yaml ä¼šåˆ›å»ºå®ƒï¼Œä½†æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„ï¼‰
echo ">>> Removing official Deployment (we use custom one)..."
kubectl delete deployment -n argocd argocd-image-updater --ignore-not-found=true
sleep 2

# åº”ç”¨è‡ªå®šä¹‰çš„ Deploymentï¼ˆä¿®å¤äº† command/args é—®é¢˜ï¼‰
echo ">>> Applying custom ArgoCD Image Updater Deployment..."
kubectl apply -f system/argocd-image-updater-controller.yaml

echo ">>> Waiting for Image Updater to be ready..."
sleep 10
kubectl get pods -n argocd | grep image-updater || echo "âš  Image Updater pods may still be starting..."

# ------------------------------------------------
# Step 3.6: å®‰è£… Monitoring Stack (Helm)
# ------------------------------------------------
echo "===== Step 3.6: Install Monitoring Stack (Helm) ====="

# ç¡®ä¿ Helm repo å·²æ·»åŠ ï¼ˆStep 1.5 å·²æ·»åŠ ï¼Œè¿™é‡Œå†æ¬¡ç¡®è®¤ï¼‰
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts || true
helm repo add nvidia https://nvidia.github.io/dcgm-exporter/helm-charts || true
helm repo update

# å®‰è£… kube-prometheus-stack (Grafana + Prometheus)
echo ">>> Installing kube-prometheus-stack..."
helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
  -n monitoring \
  --create-namespace \
  -f "${REPO_DIR}/helm/monitoring/kps-values.yaml" \
  --wait --timeout 10m || \
  echo "âš  kube-prometheus-stack installation may still be in progress..."

# å®‰è£… DCGM exporter (GPU metrics)
echo ">>> Installing dcgm-exporter..."
helm upgrade --install dcgm nvidia/dcgm-exporter \
  -n monitoring \
  -f "${REPO_DIR}/helm/monitoring/dcgm/values.yaml" \
  --wait --timeout 5m || \
  echo "âš  dcgm-exporter installation may still be in progress..."

echo ">>> Checking monitoring pods..."
kubectl get pods -n monitoring || echo "âš  Monitoring pods may still be starting..."

# ------------------------------------------------
# Step 4: çŠ¶æ€æ£€æŸ¥
# ------------------------------------------------
kubectl get runtimeclass
kubectl get nodes -o wide
kubectl get pods -A -o wide

echo ""
echo "ğŸ‰ GPU-ready Kubernetes cluster bootstrap å®Œæˆ"